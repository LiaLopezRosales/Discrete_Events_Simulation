{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problema:Insurance Risk Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import heapq\n",
    "import seaborn as sns\n",
    "from scipy.stats import chi2_contingency\n",
    "from Insurance import InsuranceRiskSimulation\n",
    "from statsmodels.stats.proportion import proportion_confint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parámetros de la simulación utilizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'nu': 0.2,    \n",
    "    'mu': 0.05,     \n",
    "    'lambd': 0.3,  \n",
    "    'c': 11,      \n",
    "    'a0': 100,   \n",
    "    'n0': 5,      \n",
    "    'T': 365,      \n",
    "    'claim_dist': lambda: np.random.exponential(scale=30)  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Estimación de la probabilidad de bancarrota(capital negativo antes de T) con 95% de seguridad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilidad estimada de no acabar en capital negativo: 100.00%\n",
      "Intervalo de Confianza 95%: [96.30%, 100.00%]\n",
      "Simulaciones realizadas: 100\n"
     ]
    }
   ],
   "source": [
    "def estimar_prob_precision_d(d, params, batch_size=100):\n",
    "    resultados = []\n",
    "    n = 0\n",
    "    while True:\n",
    "        # Ejecutar un lote de simulaciones\n",
    "        for _ in range(batch_size):\n",
    "            sim = InsuranceRiskSimulation(**params)\n",
    "            resultados.append(sim.run())\n",
    "            n += 1\n",
    "        \n",
    "        # Calcular estadísticas cada 30 simulaciones \n",
    "        if n >= 30:\n",
    "            p_hat = np.mean(resultados)\n",
    "            s_cuadrado = np.var(resultados, ddof=1)  # S² insesgado\n",
    "            error_estandar = np.sqrt(s_cuadrado / n)\n",
    "            \n",
    "            if error_estandar < d:\n",
    "                break\n",
    "    ci_low, ci_high = proportion_confint(sum(resultados), n, method='wilson')\n",
    "    return p_hat, (ci_low, ci_high), n\n",
    "\n",
    "p_hat, (ci_low, ci_high), n = estimar_prob_precision_d(\n",
    "    d=0.005, \n",
    "    params=params, \n",
    "    batch_size=100\n",
    ")\n",
    "print(f\"Probabilidad estimada de no acabar en capital negativo: {p_hat:.2%}\")\n",
    "print(f\"Intervalo de Confianza 95%: [{ci_low:.2%}, {ci_high:.2%}]\")\n",
    "print(f\"Simulaciones realizadas: {n}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función para correr y recolectar información de n simulaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_multiple_simulations(params, n_simulations=1000):\n",
    "    results = []\n",
    "    ruin_times = []\n",
    "    for _ in range(n_simulations):\n",
    "        sim = InsuranceRiskSimulation(**params)\n",
    "        result = sim.run()\n",
    "        results.append(result)\n",
    "        if result == 0:\n",
    "            ruin_times.append(sim.ruin_time)\n",
    "    return results, ruin_times\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis Exploratorio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de sensibilidad para parámetros de forma independiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de sensibilidad para un parámetro\n",
    "def sensitivity_analysis(param_name, param_values, base_params, n_simulations=1000):\n",
    "    ruin_probs = []\n",
    "    lowers = []\n",
    "    uppers = []\n",
    "    \n",
    "    for value in param_values:\n",
    "        modified_params = base_params.copy()\n",
    "        modified_params[param_name] = value\n",
    "        results, _ = run_multiple_simulations(modified_params, n_simulations)\n",
    "        \n",
    "        n_ruins = len([r for r in results if r == 0])\n",
    "        ruin_prob = n_ruins / n_simulations\n",
    "        ci_low, ci_upp = proportion_confint(n_ruins, n_simulations, method='wilson')\n",
    "        \n",
    "        ruin_probs.append(ruin_prob)\n",
    "        lowers.append(ruin_prob - ci_low)\n",
    "        uppers.append(ci_upp - ruin_prob)\n",
    "    \n",
    "    return ruin_probs, lowers, uppers\n",
    "\n",
    "\n",
    "def full_sensitivity_analysis(base_params, n_simulations=500):\n",
    "    \"\"\"\n",
    "    Realiza análisis de sensibilidad para todos los parámetros principales\n",
    "    \n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # 1. Sensibilidad a ν (tasa de llegada de nuevos clientes)\n",
    "    nu_values = np.linspace(0.05, 0.5, 10)\n",
    "    results['nu'] = sensitivity_analysis('nu', nu_values, base_params, n_simulations)\n",
    "    \n",
    "    # 2. Sensibilidad a μ (tasa de abandono de clientes)\n",
    "    mu_values = np.linspace(0.01, 0.3, 10)\n",
    "    results['mu'] = sensitivity_analysis('mu', mu_values, base_params, n_simulations)\n",
    "    \n",
    "    # 3. Sensibilidad a λ (tasa de reclamos por cliente)\n",
    "    lambda_values = np.linspace(0.01, 0.2, 10)\n",
    "    results['lambda'] = sensitivity_analysis('lambd', lambda_values, base_params, n_simulations)\n",
    "    \n",
    "    # 4. Sensibilidad a c (ingreso por cliente)\n",
    "    c_values = np.linspace(0.5, 2.0, 10)\n",
    "    results['c'] = sensitivity_analysis('c', c_values, base_params, n_simulations)\n",
    "    \n",
    "    # 5. Sensibilidad a a0 (capital inicial)\n",
    "    a0_values = np.linspace(10, 200, 10)\n",
    "    results['a0'] = sensitivity_analysis('a0', a0_values, base_params, n_simulations)\n",
    "    \n",
    "    # 6. Sensibilidad a n0 (clientes iniciales)\n",
    "    n0_values = np.linspace(1, 50, 10)\n",
    "    results['n0'] = sensitivity_analysis('n0', n0_values, base_params, n_simulations)\n",
    "    \n",
    "    # 7. Sensibilidad a T (horizonte temporal)\n",
    "    T_values = np.linspace(10, 200, 10)\n",
    "    results['T'] = sensitivity_analysis('T', T_values, base_params, n_simulations)\n",
    "    \n",
    "    # Visualización de todos los resultados\n",
    "    plot_all_sensitivity_results(results)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def plot_all_sensitivity_results(results):\n",
    "    \"\"\"Visualiza todos los resultados del análisis de sensibilidad\"\"\"\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    \n",
    "    params_order = ['nu', 'mu', 'lambda', 'c', 'a0', 'n0', 'T']\n",
    "    titles = {\n",
    "        'nu': 'Tasa de llegada de clientes (ν)',\n",
    "        'mu': 'Tasa de abandono de clientes (μ)',\n",
    "        'lambda': 'Tasa de reclamos por cliente (λ)',\n",
    "        'c': 'Ingreso por cliente (c)',\n",
    "        'a0': 'Capital inicial (a₀)',\n",
    "        'n0': 'Clientes iniciales (n₀)',\n",
    "        'T': 'Horizonte temporal (T)'\n",
    "    }\n",
    "    \n",
    "    for i, param in enumerate(params_order, 1):\n",
    "        plt.subplot(3, 3, i)\n",
    "        values = np.linspace(0, 1, 10) \n",
    "        ruin_probs, lowers, uppers = results[param]\n",
    "        \n",
    "        plt.errorbar(values, ruin_probs, yerr=[lowers, uppers], \n",
    "                    fmt='-o', capsize=5, label=param)\n",
    "        plt.title(titles[param])\n",
    "        plt.xlabel('Valor del parámetro (normalizado)')\n",
    "        plt.ylabel('Probabilidad de ruina')\n",
    "        plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "sensitivity_results = full_sensitivity_analysis(params, n_simulations=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de sensibilidad para la distribución del parámetro de reclamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def claim_distribution_sensitivity(base_params, n_simulations=500):\n",
    "    \"\"\"Analiza sensibilidad a diferentes distribuciones de reclamos\"\"\"\n",
    "    distributions = {\n",
    "        'Exponencial (media=10)': lambda: np.random.exponential(scale=10),\n",
    "        'Pareto (forma=2, escala=5)': lambda: np.random.pareto(2) * 5,\n",
    "        'Lognormal (μ=2, σ=0.5)': lambda: np.random.lognormal(2, 0.5),\n",
    "        'Normal truncada (μ=10, σ=3)': lambda: max(0, np.random.normal(10, 3))\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for name, dist in distributions.items():\n",
    "        params = base_params.copy()\n",
    "        params['claim_dist'] = dist\n",
    "        sim_results, _ = run_multiple_simulations(params, n_simulations)\n",
    "        ruin_prob = sum(1 for r in sim_results if r == 0)/len(sim_results)\n",
    "        results[name] = ruin_prob\n",
    "    \n",
    "    # Visualización\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    names = list(results.keys())\n",
    "    values = list(results.values())\n",
    "    plt.barh(names, values, color=sns.color_palette(\"husl\", len(names)))\n",
    "    \n",
    "    for i, v in enumerate(values):\n",
    "        plt.text(v + 0.01, i, f\"{v:.3f}\", color='black', va='center')\n",
    "    \n",
    "    plt.title('Probabilidad de Ruina por Distribución de Reclamos')\n",
    "    plt.xlabel('Probabilidad de Ruina')\n",
    "    plt.ylabel('Distribución de Reclamos')\n",
    "    plt.xlim(0, max(values) * 1.2)\n",
    "    plt.grid(axis='x')\n",
    "    plt.show()\n",
    "    \n",
    "    return results\n",
    "\n",
    "claim_results = claim_distribution_sensitivity(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de tiempos de bancarrota "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ruin_time_analysis(base_params, n_simulations=1000):\n",
    "    \n",
    "    results, ruin_times = run_multiple_simulations(base_params, n_simulations)\n",
    "    ruin_prob = len(ruin_times)/n_simulations\n",
    "    \n",
    "    if not ruin_times:\n",
    "        print(f\"No se observaron ruinas en {n_simulations} simulaciones\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Probabilidad de ruina: {ruin_prob:.3f}\")\n",
    "    print(f\"Tiempo medio de ruina: {np.mean(ruin_times):.1f} (mediana: {np.median(ruin_times):.1f})\")\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # 1. Histograma\n",
    "    plt.subplot(1, 3, 1)\n",
    "    sns.histplot(ruin_times, kde=True, bins=20, color='skyblue')\n",
    "    plt.title('Distribución de Tiempos de Ruina')\n",
    "    plt.xlabel('Tiempo')\n",
    "    plt.ylabel('Frecuencia')\n",
    "    \n",
    "    # 2. Función de supervivencia empírica\n",
    "    plt.subplot(1, 3, 2)\n",
    "    sorted_times = np.sort(ruin_times)\n",
    "    survival = 1 - np.arange(1, len(sorted_times)+1)/len(sorted_times)\n",
    "    plt.step(sorted_times, survival, where='post', color='orange')\n",
    "    plt.title('Función de Supervivencia Empírica')\n",
    "    plt.xlabel('Tiempo')\n",
    "    plt.ylabel('Probabilidad de Supervivencia')\n",
    "    plt.axhline(y=1-ruin_prob, color='r', linestyle='--')\n",
    "    \n",
    "    # 3. Hazard rate empírico\n",
    "    plt.subplot(1, 3, 3)\n",
    "    time_bins = np.linspace(0, base_params['T'], 20)\n",
    "    bin_counts, _ = np.histogram(ruin_times, bins=time_bins)\n",
    "    hazard_rate = bin_counts / (n_simulations * (time_bins[1]-time_bins[0]))\n",
    "    plt.bar(time_bins[:-1], hazard_rate, width=time_bins[1]-time_bins[0], alpha=0.7, color='green')\n",
    "    plt.title('Tasa de Riesgo Empírica')\n",
    "    plt.xlabel('Tiempo')\n",
    "    plt.ylabel('Tasa de Ruina Instantánea')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "ruin_time_analysis(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploración y comprobación de otros resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcular constante de ingreso(c) necesaria para tener probabilidad de bancarrota 0%(dado la configuración del resto de parámetros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_minimo(target_prob, params_base, c_min, c_max, tol=0.1, n_sim=500):\n",
    "    while c_max - c_min > tol:\n",
    "        c_medio = (c_min + c_max) / 2\n",
    "        params = params_base.copy()\n",
    "        params['c'] = c_medio\n",
    "        prob = sum(InsuranceRiskSimulation(**params).run() for _ in range(n_sim)) / n_sim\n",
    "        if prob >= target_prob:\n",
    "            c_max = c_medio\n",
    "        else:\n",
    "            c_min = c_medio\n",
    "    return c_max\n",
    "\n",
    "c_min = c_minimo(0.95, params, 50, 200)\n",
    "print(f\"Ingreso mínimo requerido: {c_min:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba de Hipótesis #1 La distribución de reclamos es influyente en la probabilidad de ruina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def claim_distribution_hypothesis_test(base_params, n_simulations=10000):\n",
    "    \"\"\"\n",
    "    Prueba si diferentes distribuciones de reclamos producen diferentes probabilidades de ruina.\n",
    "    \"\"\"\n",
    "    distributions = {\n",
    "        'Exponencial': lambda: np.random.exponential(scale=10),\n",
    "        'Pareto': lambda: np.random.pareto(2) * 5,\n",
    "        'Normal': lambda: max(0, np.random.normal(10, 3))\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for name, dist in distributions.items():\n",
    "        params = base_params.copy()\n",
    "        params['claim_dist'] = dist\n",
    "        sim_results, _ = run_multiple_simulations(params, n_simulations)\n",
    "        results[name] = sim_results\n",
    "    \n",
    "    # Prueba chi-cuadrado para proporciones\n",
    "    contingency_table = []\n",
    "    for name, res in results.items():\n",
    "        ruins = sum(1 for r in res if r == 0)\n",
    "        non_ruins = len(res) - ruins\n",
    "        contingency_table.append([ruins, non_ruins])\n",
    "    \n",
    "    chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "    print(f\"\\nChi-square Test: χ² = {chi2:.3f}, p-value = {p:.4f}\")\n",
    "    \n",
    "    if p < 0.05:\n",
    "        print(\"Existen diferencias significativas entre las distribuciones\")\n",
    "        \n",
    "        # Comparaciones por pares con corrección Bonferroni\n",
    "        print(\"\\nComparaciones por pares (Chi-square ajustado):\")\n",
    "        dist_names = list(distributions.keys())\n",
    "        for i in range(len(dist_names)):\n",
    "            for j in range(i+1, len(dist_names)):\n",
    "                obs = np.array([contingency_table[i], contingency_table[j]])\n",
    "                chi2, p, _, _ = chi2_contingency(obs)\n",
    "                print(f\"{dist_names[i]} vs {dist_names[j]}: p = {p:.4f}\")\n",
    "    else:\n",
    "        print(\"No hay diferencias significativas entre las distribuciones\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "claim_dist_results = claim_distribution_hypothesis_test(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba de Hipótesis #2 La probabilidad del tiempo de bancarrota sigue distribución exponencial o lognormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chisquare, kstest\n",
    "\n",
    "\n",
    "def goodness_of_fit_ruin_times_final(base_params, n_simulations=1000, theoretical_dist='expon'):\n",
    "    # Obtener tiempos de ruina empíricos\n",
    "    _, ruin_times = run_multiple_simulations(base_params, n_simulations)\n",
    "    ruin_times = np.array(ruin_times)\n",
    "    \n",
    "    if len(ruin_times) < 30:\n",
    "        print(f\"Error: Solo {len(ruin_times)} eventos de ruina. Mínimo requerido: 30\")\n",
    "        return None\n",
    "    \n",
    "    # Ajustar parámetros de la distribución teórica\n",
    "    if theoretical_dist == 'expon':\n",
    "        loc, scale = 0, np.mean(ruin_times)\n",
    "        cdf = lambda x: stats.expon.cdf(x, loc, scale)\n",
    "    elif theoretical_dist == 'weibull':\n",
    "        params = stats.weibull_min.fit(ruin_times, floc=0)\n",
    "        cdf = lambda x: stats.weibull_min.cdf(x, *params)\n",
    "    elif theoretical_dist == 'lognorm':\n",
    "        params = stats.lognorm.fit(ruin_times, floc=0)\n",
    "        cdf = lambda x: stats.lognorm.cdf(x, *params)\n",
    "    \n",
    "    # Determinar bins usando cuantiles empíricos\n",
    "    n_bins = max(3, min(10, len(ruin_times)//20))  # Entre 3 y 10 bins\n",
    "    bins = np.unique(np.quantile(ruin_times, np.linspace(0, 1, n_bins + 1)))\n",
    "    \n",
    "    # Calcular frecuencias observadas y esperadas\n",
    "    observed, _ = np.histogram(ruin_times, bins=bins)\n",
    "    expected = len(ruin_times) * np.diff(cdf(bins))\n",
    "    \n",
    "    # Combinar bins con frecuencias esperadas <5 de forma segura\n",
    "    i = 0\n",
    "    while i < len(expected):\n",
    "        if expected[i] < 5 and len(expected) > 1:\n",
    "            # Combinar con el siguiente bin\n",
    "            new_obs = observed[i] + observed[i+1]\n",
    "            new_exp = expected[i] + expected[i+1]\n",
    "            \n",
    "            observed = np.concatenate([observed[:i], [new_obs], observed[i+2:]])\n",
    "            expected = np.concatenate([expected[:i], [new_exp], expected[i+2:]])\n",
    "            bins = np.concatenate([bins[:i+1], bins[i+2:]])\n",
    "        else:\n",
    "            i += 1\n",
    "    \n",
    "    # Asegurar suma exacta de frecuencias esperadas\n",
    "    total_obs = np.sum(observed)\n",
    "    expected = expected * total_obs / np.sum(expected)  # Normalizar\n",
    "    \n",
    "    # Verificación final de condiciones\n",
    "    if len(expected) < 3 or np.any(expected < 5):\n",
    "        print(\"No se cumplen los supuestos del test chi-cuadrado\")\n",
    "        print(\"Usando solo prueba KS\")\n",
    "        chi2_stat, p_value = np.nan, np.nan\n",
    "    else:\n",
    "        try:\n",
    "            chi2_stat, p_value = chisquare(observed, expected)\n",
    "        except ValueError as e:\n",
    "            print(f\"Error en chi-cuadrado: {str(e)}\")\n",
    "            print(\"Realizando ajuste de precisión...\")\n",
    "            # Ajuste final de precisión\n",
    "            observed = observed.astype(np.float64)\n",
    "            expected = expected.astype(np.float64)\n",
    "            observed_sum = observed.sum()\n",
    "            expected = expected * observed_sum / expected.sum()\n",
    "            chi2_stat, p_value = chisquare(observed, expected)\n",
    "    \n",
    "    # Resultados\n",
    "    print(f\"\\nChi-square test ({theoretical_dist}):\")\n",
    "    print(f\"χ² = {chi2_stat:.3f}, p = {p_value:.4f}\")\n",
    "    print(f\"Bins: {len(bins)-1}, Observados: {observed}, Esperados: {expected.round(2)}\")\n",
    "    \n",
    "    # Prueba KS\n",
    "    ks_stat, ks_p = kstest(ruin_times, cdf)\n",
    "    print(f\"\\nKS test: D = {ks_stat:.3f}, p = {ks_p:.4f}\")\n",
    "    \n",
    "    # Visualización\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(np.arange(len(observed)), observed, width=0.4, label='Observado')\n",
    "    plt.bar(np.arange(len(expected)) + 0.4, expected, width=0.4, label='Esperado')\n",
    "    plt.xticks(np.arange(len(expected)), [f\"Bin {i+1}\" for i in range(len(expected))])\n",
    "    plt.title('Distribución de Frecuencias')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'chi2': (chi2_stat, p_value),\n",
    "        'ks': (ks_stat, ks_p),\n",
    "        'bins': bins,\n",
    "        'observed': observed,\n",
    "        'expected': expected\n",
    "    }\n",
    "\n",
    "\n",
    "gof_results = goodness_of_fit_ruin_times_final(params, theoretical_dist='expon')\n",
    "gof_results = goodness_of_fit_ruin_times_final(params, theoretical_dist='lognorm')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
